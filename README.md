<p align="center">
  <img src="media/demo.gif" alt="Demo" width="600"/>
</p>

# ğŸ§  Beauty & Electronics Chat Assistant (RAG-based LLM)

A domain-specific **AI shopping assistant** for Beauty and Electronics products, built using a full **Retrieval-Augmented Generation (RAG)** pipeline.

The system understands user shopping queries like:

> *â€œRecommend a facial cleansing tool under 20â‚¬â€*
> *â€œBest earphones under 20â‚¬â€*
> *â€œFind a good camera for wedding photographyâ€*

It then retrieves real product information from the dataset and generates natural, grounded answers using a local LLM.

---

# ğŸš€ Features

### **ğŸ” Retrieval-Augmented Generation (RAG)**

* Vector search over product corpus (FAISS)
* Accurate chunk-based retrieval using **token-based splitting**
* Domain-aware (Beauty/Electronics) product matching
* Supports mixed-domain queries

---

### **ğŸ§  Smart Query Parsing**

* Extracts:

  * **Price constraints:** â€œunder 30â€, â€œbetween 50â€“100â€, â€œabove 20â€
  * **Rating constraints:** â€œabove 4.3 starsâ€, â€œ4+ ratedâ€
  * **Desired number of products:** â€œrecommend 3 serumsâ€
* Domain detection using keyword lists
* Domain routing to Beauty/Electronics FAISS indices

---

### **âš¡ Fast & Accurate Embeddings**

* Model: **BAAI/bge-small-en-v1.5** (preferred)
* Embeddings normalized for cosine similarity
* Fast GPU encoding via SentenceTransformers

---

### **ğŸ¤– Local LLM Generation**

* Default: **Mistral-7B-Instruct-v0.2** (quantized)
* Optional: FLAN-T5-Large baseline
* Structured shopping-assistant prompt
* Generates concise, grounded recommendations
* Each result includes:

  * Product name
  * Price
  * Rating
  * Reason for recommendation

---

### **ğŸŒ Web App**

* Built with Flask
* Includes:

  * Input box for user query
  * Model-generated recommendations
  * `/api/search` JSON endpoint
  * Caching layer for repeated queries

---

# ğŸ§© Architecture

```
User Query
   â”‚
   â–¼
[ Query Parser ]
   â”‚â€” Extracts domain, price filters, rating filters, requested count
   â–¼
[ Embed Query with BGE-small ]
   â–¼
[ FAISS Retrieval ]
   â”‚â€” Beauty index
   â”‚â€” Electronics index
   â”‚â€” Global index
   â–¼
Top-k Product Chunks
   â–¼
[ LLM Generator: Mistral 7B ]
   â”‚â€” Builds product-aware prompt
   â–¼
Final Answer
```

---

# ğŸ›  Tech Stack

| Component    | Choice                               |
| ------------ | ------------------------------------ |
| Language     | Python                               |
| Embeddings   | BGE-small (`BAAI/bge-small-en-v1.5`) |
| Vector Store | FAISS                                |
| Chunking     | tiktoken (`cl100k_base`)             |
| LLM          | Mistral-7B-Instruct-v0.2 (quantized) |
| Parsing      | spaCy                                |
| Web          | Flask                                |
| Storage      | Parquet + joblib                     |

---

# ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ worked/
â”‚   â”‚   â”œâ”€â”€ cleaned_full_corpus.parquet
â”‚   â”‚   â”œâ”€â”€ cleaned_corpus_rich.parquet
â”‚   â”‚   â”œâ”€â”€ chunks.parquet
â”‚   â””â”€â”€ tokenized/
â”‚       â”œâ”€â”€ faiss_all.index
â”‚       â”œâ”€â”€ faiss_beauty.index
â”‚       â”œâ”€â”€ faiss_electronics.index
â”‚       â”œâ”€â”€ chunks_embeddings.joblib
â”‚       â””â”€â”€ meta.joblib
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tokenization_embeddings.py
â”‚   â”œâ”€â”€ retrieve.py
â”‚   â”œâ”€â”€ generate_mistral.py
â”‚   â”œâ”€â”€ generate_flan.py
â”‚   â”œâ”€â”€ model_manager.py
â”‚   â”œâ”€â”€ paths.py
â”‚   â””â”€â”€ testing_querys.py
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â””â”€â”€ README.md
```

---

# ğŸ”ª Data Processing

### **1. Cleaning**

Each record is normalized, cleaned, and merged into a single text field:

```python
parts = [
    f"Title: {row['title']}",
    f"Features: {row['features']}",
    f"Description: {row['description']}"
]
combined_text = ". ".join(str(p) for p in parts if p)
```

---

### **2. Chunking**

Token-based chunking using tiktoken:

* `chunk_size=200`
* `chunk_overlap=30`

Each chunk contains:

* doc_id
* chunk_id
* domain
* price
* rating
* categories
* title
* text (chunk body)

---

### **3. Embeddings**

Using **BAAI/bge-small-en-v1.5**:

* Fast & accurate for semantic search
* Embeddings normalized
* batched GPU encoding
* saved as numpy array via joblib

---

### **4. FAISS Indices**

Three FAISS indices:

* **faiss_all.index**
* **faiss_beauty.index**
* **faiss_electronics.index**

Plus domain index maps stored in `meta.joblib`.

---

# ğŸ” Retrieval Logic

### Steps:

1. Detect domain(s)
2. Parse price/rating filters
3. Encode user query
4. Search domain-specific FAISS index
5. Apply filters
6. Fallback if low recall
7. Return top N products with metadata

Supports:

* Mixed-domain queries
* Numeric filtering
* Domain balancing
* De-duplication

---

# ğŸ¤– LLM Generation

Mistral-7B-Instruct-v0.2 (quantized):

* Loaded with `ModelManager`
* Device-mapped automatically
* Builds product-aware prompt:

```
You are a shopping assistant...
Below are the relevant products...
Recommend exactly N...
```

Generates coherent, grounded product recommendations.

---

# ğŸŒ Web App (Flask)

Routes:

| Route         | Description        |
| ------------- | ------------------ |
| `/`           | User interface     |
| `/api/search` | JSON API           |
| `/health`     | Model health check |

Includes:

* Query caching
* FAISS retriever instance
* Mistral generator instance

---

# ğŸ§ª Example Query

```
recommend a facial cleansing tool under 20
```

LLM output example:

> **Top options under 20â‚¬:**
> â€¢ Based on your request for a facial cleansing tool under 20 euros, I would recommend Product 3: Facial Cleansing Pads, Silicone Face Scrubbers Soft and Gentle....

---

# ğŸ“ˆ Evaluation

### Qualitative

* Run `testing_querys.py`
* Inspect retrieval logs
* Inspect generated answers

### Optional Quantitative

* Manually annotate relevant products for 20â€“50 queries
* Compute **Recall@K** for retrieval

---

# ğŸ›‘ Limitations

* No review-based embeddings yet
* Domain detection is keyword-based
* Mistral 7B may hallucinate with poor prompts
* Chunk size affects recallâ€”needs tuning

---

# âœ” Installation

```
pip install -r requirements.txt
```
#### Download Datasets
Beauty dataset here (data\raw_meta_All_Beauty)
* [Beauty dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/blob/main/raw_meta_All_Beauty/full-00000-of-00001.parquet)

Electronics dataset here (data\raw_meta_Electronics)
* [Electronics dataset 1](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/blob/main/raw_meta_Electronics/full-00000-of-00010.parquet)
* [Electronics dataset 2](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/blob/main/raw_meta_Electronics/full-00001-of-00010.parquet)

---

# âœ” Build Indices

```
python -m src.tokenization_embeddings
```

---

# âœ” Run App

```
python app.py
```

Open browser:

```
http://127.0.0.1:5000/
```
---

# ğŸ¤ Contributing

Contributions are welcome!
If youâ€™d like to improve the retrieval pipeline, add new models, optimize the web UI, or fix bugs, feel free to submit a Pull Request or open an issue.

---

# ğŸ“„ License

This project is licensed under the **MIT License** â€” see the `LICENSE` file for details.

---

# ğŸ‘¨â€ğŸ’» Author

**Abdul Haseeb**

* GitHub: [@Haseeb2510](https://github.com/Haseeb2510)
* LinkedIn: [Abdul Haseeb](https://www.linkedin.com/in/abdul-haseeb-172542243)

---

---

# ğŸ‰ Acknowledgments

This project was made possible thanks to the open-source community and the powerful tools that support modern NLP, vector search, and LLM development.

Special thanks to:

* **MCAuley Lab (UC San Diego)** â€” for maintaining the Amazon product review and metadata datasets, which enabled high-quality product information retrieval for this project. Their long-standing research contributions to recommendation systems and product modeling made this work possible.
* **BAAI Research** â€” for the *BGE embedding models* used in semantic retrieval.
* **Mistral AI** â€” for the *Mistral-7B-Instruct* model powering natural-language recommendations.
* **SentenceTransformers team** â€” for the embedding framework used to encode product chunks efficiently.
* **FAISS (Meta AI)** â€” for the high-performance vector indexing library.
* **spaCy** â€” for fast, reliable NLP parsing and token extraction.
* **tiktoken** â€” for efficient tokenization used in the chunking stage.
* **Flask** â€” for the lightweight and flexible web framework powering the UI and API.
* **Pandas, NumPy, and PyArrow** â€” for powering all data cleaning, storage, and transformations.
* **The open-source ecosystem** â€” for maintaining all the tools, libraries, and models that made this end-to-end RAG system accessible, reproducible, and high-quality.

Grateful to the entire community for enabling this project.

