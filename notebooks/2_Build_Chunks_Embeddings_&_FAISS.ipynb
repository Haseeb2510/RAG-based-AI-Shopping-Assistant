{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd557481",
   "metadata": {},
   "source": [
    "# üìì **02 - Build Chunks, Embeddings & FAISS Indices**\n",
    "\n",
    "This notebook performs the crucial step of transforming our cleaned product data into `chunks`, `embeddings` and `faiss indices` that'll be used later by our RAG. We'll convert text into numerical embeddings and build efficient indices for semantic search.\n",
    "\n",
    "### **Objectives of this notebook**\n",
    "\n",
    "* **Chunk** the cleaned product descriptions into manageable text segments using token-based overlapping windows\n",
    "* **Generate embeddings** using Sentence Transformers to create dense vector representations\n",
    "* **Build FAISS indices** for fast similarity search across different domains\n",
    "* **Validate** the retrieval system with sample queries to ensure quality\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Pipeline Overview**\n",
    "\n",
    "The vector store construction follows this workflow:\n",
    "\n",
    "1. **Load Cleaned Data** ‚Üí Read the preprocessed corpus from `cleaned_full_corpus.parquet`\n",
    "2. **Text Chunking** ‚Üí Split documents into overlapping token-based chunks\n",
    "3. **Embedding Generation** ‚Üí Convert text chunks to dense vector representations using Sentence Transformers\n",
    "4. **Index Building** ‚Üí Create FAISS indices for efficient similarity search\n",
    "5. **Metadata Storage** ‚Üí Save domain masks and mapping information for filtered retrieval\n",
    "\n",
    "---\n",
    "\n",
    "## **Import & Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864e2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make src importable by adding project root to Python path\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "from src.paths import WORKED_FOLDER, TOKENIZATION_DATA, CHUNKS_PATH, EMBEDDINGS_PATH\n",
    "\n",
    "# Load cleaned_full_corpus.parquet, created in the previous notebook\n",
    "df = pd.read_parquet(os.path.join(WORKED_FOLDER, \"cleaned_full_corpus.parquet\"))\n",
    "df['doc_id'] = np.arange(len(df), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d5667",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.üß† **The Chunking Process**\n",
    "\n",
    "### **Why Chunking is Necessary**\n",
    "\n",
    "Product descriptions in our dataset can be quite lengthy. Chunking serves several important purposes:\n",
    "\n",
    "* **Context Management**: LLMs and embedding models have limited context windows\n",
    "* **Precision**: Smaller chunks allow more targeted retrieval of relevant information\n",
    "* **Overlap Preservation**: 30-token overlap ensures we don't lose context at chunk boundaries\n",
    "* **Efficiency**: Smaller chunks are faster to process and embed\n",
    "\n",
    "### **Token-Based vs Character-Based Chunking**\n",
    "\n",
    "Our pipeline uses **token-based chunking** with OpenAI's `tiktoken` tokenizer, which offers several advantages:\n",
    "\n",
    "* **Consistent with LLMs**: Uses the same tokenization as GPT models\n",
    "* **Language Agnostic**: Handles different languages and special characters better than character counts\n",
    "* **Meaningful Units**: Tokens correspond more closely to semantic units than characters\n",
    "\n",
    "### **Chunking Parameters**\n",
    "\n",
    "* **Chunk Size**: 200 tokens - balances context richness with precision\n",
    "* **Chunk Overlap**: 30 tokens - preserves context across chunk boundaries\n",
    "* **Tokenizer**: `cl100k_base` encoding (same as GPT-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ca4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of what chunking achieves:\n",
    "original_text = \"This is a long product description that needs to be split into smaller pieces for better retrieval.\"\n",
    "# After chunking (simplified):\n",
    "chunk1 = \"This is a long product description that needs to be\"\n",
    "chunk2 = \"description that needs to be split into smaller pieces\"\n",
    "chunk3 = \"into smaller pieces for better retrieval.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593af642",
   "metadata": {},
   "source": [
    "### Creating chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b8cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading existing chunks file...\n"
     ]
    }
   ],
   "source": [
    "# 2) Create chunks.parquet (if not exists) using token-based chunking\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "def chunk_text(text: str, enc: tiktoken.Encoding, chunk_size: int=200, chunk_overlap: int=30) -> list:\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk = tokens[i: i + chunk_size]\n",
    "        if not chunk:\n",
    "            continue\n",
    "        chunks.append(enc.decode(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def create_chunks(df: pd.DataFrame, path=CHUNKS_PATH) -> pd.DataFrame:\n",
    "    if os.path.exists(path):\n",
    "        print(\"üìÇ Loading existing chunks file...\")\n",
    "        chunks_df = pd.read_parquet(path)\n",
    "    else:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        rows = []\n",
    "        chunk_id = 0\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking\"):\n",
    "            text = row.combined_text\n",
    "\n",
    "            chunk_list = chunk_text(\n",
    "                text=text,\n",
    "                enc=enc,\n",
    "                chunk_size=200,\n",
    "                chunk_overlap=30\n",
    "            )\n",
    "\n",
    "            for chunk in chunk_list:\n",
    "                rows.append({\n",
    "                    \"doc_id\": idx,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"domain\": row[\"domain\"],\n",
    "                    \"price\": row[\"price\"],\n",
    "                    \"average_rating\": row[\"average_rating\"],\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"categories\": row[\"categories\"],\n",
    "                    \"text\": chunk\n",
    "                })\n",
    "                chunk_id += 1\n",
    "        \n",
    "        chunks_df = pd.DataFrame(rows)\n",
    "\n",
    "        # Save metadata for later\n",
    "        chunks_df.to_parquet(path, index=False)\n",
    "        print(\"üíæ chunks_parquet saved successfully.\")\n",
    "        \n",
    "\n",
    "    return chunks_df\n",
    "chunks_df = create_chunks(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010a68b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2.üî¨ **Embedding Generation**\n",
    "\n",
    "### **What are Embeddings?**\n",
    "\n",
    "Embeddings are numerical representations of text that capture semantic meaning. Similar products will have similar embedding vectors, enabling semantic search.\n",
    "\n",
    "### **Sentence Transformer Model: BAAI/bge-base-en-v1.5**\n",
    "\n",
    "We use this model because it's:\n",
    "* **Specialized for Retrieval**: Optimized for semantic similarity tasks\n",
    "* **English-Optimized**: Trained primarily on English text\n",
    "* **High Quality**: Produces 768-dimensional vectors with strong semantic capture\n",
    "* **Efficient**: Balances performance and computational requirements\n",
    "\n",
    "### **Embedding Properties**\n",
    "\n",
    "* **Dimension**: 768 dimensions per vector\n",
    "* **Normalization**: All vectors are normalized to unit length\n",
    "* **Similarity Metric**: Cosine similarity (equivalent to inner product for normalized vectors)\n",
    "* **Storage**: float32 for FAISS compatibility and memory efficiency\n",
    "\n",
    "### **Batch Processing**\n",
    "\n",
    "* **Batch Size**: 256 texts - optimized for GPU memory if available\n",
    "* **Progress Tracking**: Shows real-time progress for large datasets\n",
    "* **Automatic Hardware Detection**: Uses GPU if available, falls back to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6243658",
   "metadata": {},
   "source": [
    "### Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094bf9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hasee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.4)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hasee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "üìÇ Loading existing embeddings...\n",
      "üî¢ Embedding shape:  (326813, 768)\n"
     ]
    }
   ],
   "source": [
    "# 2) Generate embeddings using Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "def create_embeddings(chunks_df: pd.DataFrame, model_name=\"BAAI/bge-base-en-v1.5\", path=EMBEDDINGS_PATH, force_compute=False) -> tuple[np.ndarray, str]:    \n",
    "    if os.path.exists(path) and not force_compute:\n",
    "        print(\"üìÇ Loading existing embeddings...\")\n",
    "        embeddings = joblib.load(path)\n",
    "        \n",
    "    else:\n",
    "        print(\"üî® Creating new embeddings...\")\n",
    "        model = SentenceTransformer(model_name)   # uses CPU/GPU automatically\n",
    "        BATCH_SIZE = 256                              # tune to VRAM\n",
    "\n",
    "        # Encoding\n",
    "        texts = chunks_df[\"text\"].tolist()\n",
    "        embeddings = model.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        joblib.dump(embeddings, path)\n",
    "        print(\"üíæ Embeddings saved successfully.\")\n",
    "    print(\"üî¢ Embedding shape: \", embeddings.shape)\n",
    "\n",
    "    # Save metadat for later\n",
    "    return embeddings, model_name\n",
    "\n",
    "embeddings, model_name = create_embeddings(chunks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27b76b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.üóÇÔ∏è **FAISS Index Architecture**\n",
    "\n",
    "### **Why FAISS?**\n",
    "\n",
    "Facebook AI Similarity Search (FAISS) is optimized for:\n",
    "* **Fast nearest neighbor search** even in high-dimensional spaces\n",
    "* **Memory efficiency** with large vector databases\n",
    "* **GPU acceleration** support\n",
    "* **Multiple index types** for different trade-offs\n",
    "\n",
    "### **Index Structure**\n",
    "\n",
    "We build three separate indices to enable flexible retrieval:\n",
    "\n",
    "1. **`faiss_all.index`** - Complete corpus for general search across all products\n",
    "2. **`faiss_beauty.index`** - Beauty domain products only for category-specific queries\n",
    "3. **`faiss_electronics.index`** - Electronics domain products only\n",
    "\n",
    "### **Index Configuration**\n",
    "\n",
    "* **Index Type**: `IndexFlatIP` (Inner Product)\n",
    "* **Similarity Metric**: Cosine similarity (via inner product on normalized vectors)\n",
    "* **Domain Masks**: Boolean arrays to filter chunks by product domain\n",
    "* **Metadata Mapping**: Stores chunk-to-document relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94975a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved indices + metadata to  c:\\Users\\hasee\\Documents\\Python_works\\NLP\\RAG_LLM\\data/tokenized\n"
     ]
    }
   ],
   "source": [
    "# 3) Build FAISS indices for combined and domain-specific retrieval\n",
    "# and save all artifacts to TOKENIZATION_DATA folder\n",
    "import faiss\n",
    "def build_index(embeddings: np.ndarray, chunks_df: pd.DataFrame, model_name):\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    index_all = faiss.IndexFlatIP(dim)\n",
    "    index_all.add(embeddings)                       # type: ignore\n",
    "\n",
    "    # domain masks\n",
    "    beauty_mask = chunks_df[\"domain\"] == \"Beauty\"\n",
    "    electronics_mask = chunks_df[\"domain\"] == \"Electronics\"\n",
    "\n",
    "    index_beauty = faiss.IndexFlatIP(dim)\n",
    "    index_beauty.add(embeddings[beauty_mask])       # type: ignore\n",
    "\n",
    "    index_elec = faiss.IndexFlatIP(dim)\n",
    "    index_elec.add(embeddings[electronics_mask])    # type: ignore\n",
    "\n",
    "    # Save \n",
    "    save_artifacts(index_all, index_beauty, beauty_mask, index_elec, electronics_mask, model_name, dim)\n",
    "    \n",
    "\n",
    "def save_artifacts(index_all: faiss.Index, index_beauty: faiss.Index, beauty_mask: pd.Series, index_elec: faiss.Index, \n",
    "                   electronics_mask: pd.Series, model_name: str, dim: int):\n",
    "    faiss.write_index(index_all, os.path.join(TOKENIZATION_DATA, \"faiss_all.index\"))\n",
    "    faiss.write_index(index_beauty, os.path.join(TOKENIZATION_DATA, \"faiss_beauty.index\"))\n",
    "    faiss.write_index(index_elec, os.path.join(TOKENIZATION_DATA, \"faiss_electronics.index\"))\n",
    "\n",
    "    # Map arrays\n",
    "    meta = {\n",
    "        \"model_name\": model_name,\n",
    "        \"dim\": dim,\n",
    "        \"doc_table_path\": CHUNKS_PATH,\n",
    "        \"beauty_indices\": np.where(beauty_mask)[0].astype(\"int64\"),\n",
    "        \"electronics_indices\": np.where(electronics_mask)[0].astype(\"int64\"),   \n",
    "    }\n",
    "    joblib.dump(meta, os.path.join(TOKENIZATION_DATA, \"meta.joblib\"))\n",
    "\n",
    "    print(\"üíæ Saved indices + metadata to \", TOKENIZATION_DATA)\n",
    "\n",
    "build_index(embeddings, chunks_df, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ffbda",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ **Artifacts Created**\n",
    "\n",
    "After running the pipeline, these files are generated in `TOKENIZATION_DATA`:\n",
    "\n",
    "```text\n",
    "tokenization_data/\n",
    "‚îú‚îÄ‚îÄ chunks.parquet              # Text chunks with metadata (doc_id, chunk_id, domain, price, etc.)\n",
    "‚îú‚îÄ‚îÄ embeddings.joblib           # Precomputed embeddings (float32 numpy array)\n",
    "‚îú‚îÄ‚îÄ faiss_all.index             # Combined FAISS index for all products\n",
    "‚îú‚îÄ‚îÄ faiss_beauty.index          # Beauty domain index only\n",
    "‚îú‚îÄ‚îÄ faiss_electronics.index     # Electronics domain index only\n",
    "‚îî‚îÄ‚îÄ meta.joblib                 # Metadata and domain mappings\n",
    "```\n",
    "\n",
    "\n",
    "### **Metadata Contents**\n",
    "\n",
    "The `meta.joblib` file contains:\n",
    "- `model_name`: Embedding model used\n",
    "- `dim`: Embedding dimension (768)\n",
    "- `doc_table_path`: Path to chunks dataframe\n",
    "- `beauty_indices`: NumPy array of indices belonging to Beauty domain\n",
    "- `electronics_indices`: NumPy array of indices belonging to Electronics domain\n",
    "\n",
    "## üéØ **Quality Assessment**\n",
    "\n",
    "### **What to Look For in Results**\n",
    "\n",
    "* **High Similarity Scores**: Values close to 1.0 indicate strong semantic match\n",
    "* **Domain Consistency**: Beauty queries should return beauty products, electronics should return electronics\n",
    "* **Relevant Content**: Retrieved chunks should directly address the query topic\n",
    "* **Metadata Integrity**: Prices, ratings, and titles should match the chunk content\n",
    "\n",
    "### **Common Issues to Watch For**\n",
    "\n",
    "* **Low Scores** (< 0.3): May indicate poor semantic matching or need for better chunking\n",
    "* **Cross-Domain Results**: Beauty queries returning electronics (or vice versa)\n",
    "* **Irrelevant Content**: Chunks that don't address the query despite high scores\n",
    "* **Incomplete Context**: Chunks that cut off important information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32222fb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîç **Inspect Generated Chunks**\n",
    "\n",
    "Let's examine the chunked data to understand the text segmentation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2dca4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total chunks created: 326,813\n",
      "üìù Unique documents chunked: 142,642\n",
      "Sample chunks with metadata:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>price</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>6.99</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Lurrose 100Pcs Full Cover Fake Toenails Artifi...</td>\n",
       "      <td>Other Beauty</td>\n",
       "      <td>Title: Lurrose 100Pcs Full Cover Fake Toenails...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>6.99</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Lurrose 100Pcs Full Cover Fake Toenails Artifi...</td>\n",
       "      <td>Other Beauty</td>\n",
       "      <td>with perfect length. You have the option to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>86.95</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Gold extatic Musk EDT 90ml</td>\n",
       "      <td>Other Beauty</td>\n",
       "      <td>Title: Gold extatic Musk EDT 90ml. Features: E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>79.50</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Brand New Headrang Face line Contour V-line Ma...</td>\n",
       "      <td>Skin Care</td>\n",
       "      <td>Title: Brand New Headrang Face line Contour V-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5.99</td>\n",
       "      <td>4.4</td>\n",
       "      <td>BioMiracle StarDust Pixie Bubble Mask, Clarify...</td>\n",
       "      <td>Skin Care Face Masks</td>\n",
       "      <td>Title: BioMiracle StarDust Pixie Bubble Mask, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  chunk_id  domain  price  average_rating  \\\n",
       "0       0         0  Beauty   6.99             3.7   \n",
       "1       0         1  Beauty   6.99             3.7   \n",
       "2       1         2  Beauty  86.95             3.7   \n",
       "3       2         3  Beauty  79.50             3.3   \n",
       "4       3         4  Beauty   5.99             4.4   \n",
       "\n",
       "                                               title            categories  \\\n",
       "0  Lurrose 100Pcs Full Cover Fake Toenails Artifi...          Other Beauty   \n",
       "1  Lurrose 100Pcs Full Cover Fake Toenails Artifi...          Other Beauty   \n",
       "2                         Gold extatic Musk EDT 90ml          Other Beauty   \n",
       "3  Brand New Headrang Face line Contour V-line Ma...             Skin Care   \n",
       "4  BioMiracle StarDust Pixie Bubble Mask, Clarify...  Skin Care Face Masks   \n",
       "\n",
       "                                                text  \n",
       "0  Title: Lurrose 100Pcs Full Cover Fake Toenails...  \n",
       "1   with perfect length. You have the option to w...  \n",
       "2  Title: Gold extatic Musk EDT 90ml. Features: E...  \n",
       "3  Title: Brand New Headrang Face line Contour V-...  \n",
       "4  Title: BioMiracle StarDust Pixie Bubble Mask, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the chunks dataframe\n",
    "print(f\"üìä Total chunks created: {len(chunks_df):,}\")\n",
    "print(f\"üìù Unique documents chunked: {chunks_df['doc_id'].nunique():,}\")\n",
    "\n",
    "# Display first few chunks with their metadata\n",
    "print(\"Sample chunks with metadata:\")\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7dd646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Chunk length statistics (characters):\n",
      "   Mean: 596.1\n",
      "   Std:  292.8\n",
      "   Min:  1.0\n",
      "   Max:  1308.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze chunk length distribution\n",
    "chunk_stats = chunks_df[\"text\"].str.len().describe()\n",
    "print(\"üìè Chunk length statistics (characters):\")\n",
    "print(f\"   Mean: {chunk_stats['mean']:.1f}\")\n",
    "print(f\"   Std:  {chunk_stats['std']:.1f}\")\n",
    "print(f\"   Min:  {chunk_stats['min']:.1f}\")\n",
    "print(f\"   Max:  {chunk_stats['max']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1eef499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Domain distribution in chunks:\n",
      "domain\n",
      "Electronics    303099\n",
      "Beauty          23714\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check domain distribution in chunks\n",
    "print(\"\\nüåê Domain distribution in chunks:\")\n",
    "print(chunks_df['domain'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f3cc834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíÑ Beauty domain sample chunks:\n",
      "Chunk 0: Title: Lurrose 100Pcs Full Cover Fake Toenails Artificial Transparent Nail Tips Nail Art for DIY. Fe...\n",
      "Chunk 1:  with perfect length. You have the option to wear them long or clip them short, easy to trim and fil...\n",
      "Chunk 2: Title: Gold extatic Musk EDT 90ml. Features: Extatic Balmain Gold Musk By Balmain Edt Spray 3 Oz. De...\n",
      "\n",
      "üîå Electronics domain sample chunks:\n",
      "Chunk 23714: Title: Digi-Tatoo Decal Skin Compatible With MacBook Pro 13 inch (Model A2338/ A2289/ A2251) - Prote...\n",
      "Chunk 23715:  impressive looking. Take it out and get tons of compliments. Easy Apply. Easy, bubble-free installa...\n",
      "Chunk 23716: Title: NotoCity Compatible with Vivoactive 4 band 22mm Quick Release Silicone Bands/Garmin Darth Vad...\n"
     ]
    }
   ],
   "source": [
    "# Show sample chunks from each domain\n",
    "print(\"üíÑ Beauty domain sample chunks:\")\n",
    "beauty_sample = chunks_df[chunks_df['domain'] == 'Beauty'].head(3)\n",
    "for idx, row in beauty_sample.iterrows():\n",
    "    print(f\"Chunk {row['chunk_id']}: {row['text'][:100]}...\")\n",
    "\n",
    "print(\"\\nüîå Electronics domain sample chunks:\")\n",
    "electronics_sample = chunks_df[chunks_df['domain'] == 'Electronics'].head(3)\n",
    "for idx, row in electronics_sample.iterrows():\n",
    "    print(f\"Chunk {row['chunk_id']}: {row['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ba3cd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üîé **Manual Nearest Neighbor Inspection**\n",
    "\n",
    "Let's test the retrieval system with sample queries to validate it's working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b2a5eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loaded retrieval components:\n",
      "   Model: BAAI/bge-base-en-v1.5\n",
      "   Embedding dimension: 768\n",
      "   Total chunks in index: 326813\n"
     ]
    }
   ],
   "source": [
    "# Load the FAISS index and metadata\n",
    "meta = joblib.load(os.path.join(TOKENIZATION_DATA, \"meta.joblib\"))\n",
    "index = faiss.read_index(os.path.join(TOKENIZATION_DATA, \"faiss_all.index\"))\n",
    "table = chunks_df\n",
    "model = SentenceTransformer(meta[\"model_name\"])\n",
    "\n",
    "print(\"üîß Loaded retrieval components:\")\n",
    "print(f\"   Model: {meta['model_name']}\")\n",
    "print(f\"   Embedding dimension: {meta['dim']}\")\n",
    "print(f\"   Total chunks in index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8ceff",
   "metadata": {},
   "source": [
    "### **Test Query 1: Beauty Product Search**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24abe2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'moisturizing face cream for dry skin'\n",
      "üìà Top 5 results (scores: [0.7541969  0.7456913  0.74467677 0.74290526 0.74100345])\n",
      "üî¢ Chunk IDs: [ 2979 16409  4836 21289  8635]\n",
      "\n",
      "üìã Retrieved results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>domain</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>Sebamed Moisturizing Cream, Sensitive Skin, 2....</td>\n",
       "      <td>83.95</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: Sebamed Moisturizing Cream, Sensitive S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16409</th>\n",
       "      <td>Facial Moisturizer. Preservative Free. Organic...</td>\n",
       "      <td>29.99</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: Facial Moisturizer. Preservative Free. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>Moisturising Cream, Body and Face Moisturizer ...</td>\n",
       "      <td>8.99</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: Moisturising Cream, Body and Face Moist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21289</th>\n",
       "      <td>DayTime Moisturizer for Dry Skin</td>\n",
       "      <td>70.00</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: DayTime Moisturizer for Dry Skin. Featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>Fresh Vitamin Nectar Moisture Glow Face Cream ...</td>\n",
       "      <td>19.00</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: Fresh Vitamin Nectar Moisture Glow Face...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  price  \\\n",
       "2979   Sebamed Moisturizing Cream, Sensitive Skin, 2....  83.95   \n",
       "16409  Facial Moisturizer. Preservative Free. Organic...  29.99   \n",
       "4836   Moisturising Cream, Body and Face Moisturizer ...   8.99   \n",
       "21289                   DayTime Moisturizer for Dry Skin  70.00   \n",
       "8635   Fresh Vitamin Nectar Moisture Glow Face Cream ...  19.00   \n",
       "\n",
       "       average_rating  domain  \\\n",
       "2979              5.0  Beauty   \n",
       "16409             4.3  Beauty   \n",
       "4836              4.2  Beauty   \n",
       "21289             4.1  Beauty   \n",
       "8635              3.5  Beauty   \n",
       "\n",
       "                                                    text  \n",
       "2979   Title: Sebamed Moisturizing Cream, Sensitive S...  \n",
       "16409  Title: Facial Moisturizer. Preservative Free. ...  \n",
       "4836   Title: Moisturising Cream, Body and Face Moist...  \n",
       "21289  Title: DayTime Moisturizer for Dry Skin. Featu...  \n",
       "8635   Title: Fresh Vitamin Nectar Moisture Glow Face...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test query for beauty products\n",
    "query = \"moisturizing face cream for dry skin\"\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "\n",
    "# Encode query to embedding\n",
    "q_emb = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "# Search for top 5 most similar chunks\n",
    "scores, ids = index.search(q_emb, 5)\n",
    "\n",
    "print(f\"üìà Top 5 results (scores: {scores[0]})\")\n",
    "print(f\"üî¢ Chunk IDs: {ids[0]}\")\n",
    "\n",
    "# Display retrieved results with relevant metadata\n",
    "results = table.iloc[ids[0]][[\"title\", \"price\", \"average_rating\", \"domain\", \"text\"]]\n",
    "print(\"\\nüìã Retrieved results:\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489426c0",
   "metadata": {},
   "source": [
    "### **Test Query 2: Electronics Product Search**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "839c8692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'wireless bluetooth headphones with noise cancellation'\n",
      "üìà Top 5 results (scores: [0.8025458  0.7917396  0.7910613  0.78404117 0.77654284])\n",
      "üî¢ Chunk IDs: [ 48934 183928 220356 169812 181427]\n",
      "\n",
      "üìã Retrieved results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>domain</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48934</th>\n",
       "      <td>Active Noise Cancelling Headphones,Wireless Bl...</td>\n",
       "      <td>39.31</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Title: Active Noise Cancelling Headphones,Wire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183928</th>\n",
       "      <td>Bose QuietComfort 35 (Series I) Wireless Headp...</td>\n",
       "      <td>174.95</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Title: Bose QuietComfort 35 (Series I) Wireles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220356</th>\n",
       "      <td>Sony Noise Cancelling Headphones WHCH710N: Wir...</td>\n",
       "      <td>71.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Title: Sony Noise Cancelling Headphones WHCH71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169812</th>\n",
       "      <td>Krankz Audio Noise Cancelling Bluetooth Headph...</td>\n",
       "      <td>149.95</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Title: Krankz Audio Noise Cancelling Bluetooth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181427</th>\n",
       "      <td>Bluetooth Headphones Noise-canceling Magnetic ...</td>\n",
       "      <td>16.59</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Title: Bluetooth Headphones Noise-canceling Ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title   price  \\\n",
       "48934   Active Noise Cancelling Headphones,Wireless Bl...   39.31   \n",
       "183928  Bose QuietComfort 35 (Series I) Wireless Headp...  174.95   \n",
       "220356  Sony Noise Cancelling Headphones WHCH710N: Wir...   71.25   \n",
       "169812  Krankz Audio Noise Cancelling Bluetooth Headph...  149.95   \n",
       "181427  Bluetooth Headphones Noise-canceling Magnetic ...   16.59   \n",
       "\n",
       "        average_rating       domain  \\\n",
       "48934              4.3  Electronics   \n",
       "183928             4.4  Electronics   \n",
       "220356             4.4  Electronics   \n",
       "169812             3.9  Electronics   \n",
       "181427             3.4  Electronics   \n",
       "\n",
       "                                                     text  \n",
       "48934   Title: Active Noise Cancelling Headphones,Wire...  \n",
       "183928  Title: Bose QuietComfort 35 (Series I) Wireles...  \n",
       "220356  Title: Sony Noise Cancelling Headphones WHCH71...  \n",
       "169812  Title: Krankz Audio Noise Cancelling Bluetooth...  \n",
       "181427  Title: Bluetooth Headphones Noise-canceling Ma...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test query for electronics\n",
    "query = \"wireless bluetooth headphones with noise cancellation\"\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "\n",
    "# Encode query to embedding\n",
    "q_emb = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "# Search for top 5 most similar chunks\n",
    "scores, ids = index.search(q_emb, 5)\n",
    "\n",
    "print(f\"üìà Top 5 results (scores: {scores[0]})\")\n",
    "print(f\"üî¢ Chunk IDs: {ids[0]}\")\n",
    "\n",
    "# Display retrieved results\n",
    "results = table.iloc[ids[0]][[\"title\", \"price\", \"average_rating\", \"domain\", \"text\"]]\n",
    "print(\"\\nüìã Retrieved results:\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1d142",
   "metadata": {},
   "source": [
    "### **Domain-Specific Search Test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cee1e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíÑ Domain-specific query (Beauty only): 'anti-aging serum with vitamin C'\n",
      "üìà Top 3 beauty results (scores: [0.80849946 0.8013123  0.80018973])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>domain</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>True Botanix Anti Aging Vitamin C Face Serum 3...</td>\n",
       "      <td>18.00</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: True Botanix Anti Aging Vitamin C Face ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3710</th>\n",
       "      <td>Retinol Plus Anti Aging Day cream with Retinol...</td>\n",
       "      <td>10.50</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: Retinol Plus Anti Aging Day cream with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21784</th>\n",
       "      <td>Retinol Serum Advanced Formula with Vitamin C,...</td>\n",
       "      <td>26.99</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Title: Retinol Serum Advanced Formula with Vit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  price  \\\n",
       "838    True Botanix Anti Aging Vitamin C Face Serum 3...  18.00   \n",
       "3710   Retinol Plus Anti Aging Day cream with Retinol...  10.50   \n",
       "21784  Retinol Serum Advanced Formula with Vitamin C,...  26.99   \n",
       "\n",
       "       average_rating  domain  \\\n",
       "838               4.9  Beauty   \n",
       "3710              4.4  Beauty   \n",
       "21784             3.3  Beauty   \n",
       "\n",
       "                                                    text  \n",
       "838    Title: True Botanix Anti Aging Vitamin C Face ...  \n",
       "3710   Title: Retinol Plus Anti Aging Day cream with ...  \n",
       "21784  Title: Retinol Serum Advanced Formula with Vit...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test domain-specific search using beauty-only index\n",
    "index_beauty = faiss.read_index(os.path.join(TOKENIZATION_DATA, \"faiss_beauty.index\"))\n",
    "beauty_indices = meta[\"beauty_indices\"]\n",
    "beauty_table = table.iloc[beauty_indices]\n",
    "\n",
    "query = \"anti-aging serum with vitamin C\"\n",
    "print(f\"üíÑ Domain-specific query (Beauty only): '{query}'\")\n",
    "\n",
    "q_emb = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "scores, local_ids = index_beauty.search(q_emb, 3)\n",
    "\n",
    "# Map back to original indices\n",
    "global_ids = beauty_indices[local_ids[0]]\n",
    "\n",
    "print(f\"üìà Top 3 beauty results (scores: {scores[0]})\")\n",
    "results = table.iloc[global_ids][[\"title\", \"price\", \"average_rating\", \"domain\", \"text\"]]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bffdc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÆ **Summary**\n",
    "\n",
    "This enables several advanced applications:\n",
    "\n",
    "* **Semantic Search**: Find products based on meaning rather than keywords\n",
    "* **Domain-Filtered Retrieval**: Search within specific product categories\n",
    "* **Hybrid Search**: Combine semantic search with metadata filtering (price, rating)\n",
    "* **RAG Applications**: Use retrieved products as context for LLM-based recommendations\n",
    "* **Similar Product Recommendations**: Find similar items based on embedding similarity\n",
    "\n",
    "The built indices are now ready for integration with your retrieval system and can be used in downstream applications like web APIs, recommendation engines, or chat interfaces.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "We've successfully transformed our cleaned product data into a powerful vector search system. The combination of thoughtful chunking, high-quality embeddings, and efficient FAISS indices creates a foundation for intelligent product discovery and recommendation.\n",
    "\n",
    "The system can now understand nuanced queries like \"affordable skincare for sensitive skin\" or \"wireless earbuds with long battery life\" and return relevant products based on semantic similarity rather than just keyword matching.\n",
    "\n",
    "And now we'll move to our next notebook where we will go through how i set-up the RAG system and Implement it with local LLM (Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5be68",
   "metadata": {},
   "source": [
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
